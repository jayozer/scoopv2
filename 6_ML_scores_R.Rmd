---
title: "scores"
output:
  html_document:
    df_print: paged
---


## Prepare the environment
```{r}
### clean the environment, and load all needed libraries
rm(list=ls())
```

```{r}
suppressWarnings(suppressMessages(library(DataExplorer))) #eda
suppressWarnings(suppressMessages(library(Hmisc))) #eda
suppressWarnings(suppressMessages(library(dplyr))) #data manipulation
suppressWarnings(suppressMessages(library(factoextra)))
suppressWarnings(suppressMessages(library(rpart))) # recursive partitioning of trees
suppressWarnings(suppressMessages(library(rpart.plot)))
suppressWarnings(suppressMessages(library(plotly))) #cool graphing
suppressWarnings(suppressMessages(library(RColorBrewer)))
suppressWarnings(suppressMessages(library(class))) # for KNN()
suppressWarnings(suppressMessages(library(gmodels))) # for model fitting such as CrossTable()
suppressWarnings(suppressMessages(library(caret))) # dummyVars and confusionMatrix()
suppressWarnings(suppressMessages(library(data.table))) #use fread() function to load data.
suppressWarnings(suppressMessages(library(ggplot2))) #to better understand data
suppressWarnings(suppressMessages(library(scales))) # to display confision matrix as ggplot graphic
suppressWarnings(suppressMessages(library(corrplot))) #correlation matrix
suppressWarnings(suppressMessages(library(randomForest))) #correlation matrix
suppressWarnings(suppressMessages(library(vcd))) #calculate Kappa for performance measures
suppressWarnings(suppressMessages(library(irr))) #kappa2() 
suppressWarnings(suppressMessages(library(rattle))) #fancyRpartPlot()


#Using suppressWarnings(suppressMessages()) to remove the warning messages. 
```

```{r}
getwd()
```

## Load data

```{r}
scores_raw <- read.csv("/cloud/project/Week7/df_features_business.csv",header=TRUE, dec = ",", stringsAsFactors = FALSE)
#confirm the load
str(scores_raw)
head(scores_raw, n=3)

#5001 observations in 9 variables. inspection_score_mean is the dependent/response/target variable. Also note that class is chr. This will need to be treated.
```
```{r}
scores_raw$inspection_score_mean <- as.numeric(as.character(scores_raw$inspection_score_mean))
scores_raw$inspection_score_min <- as.numeric(as.character(scores_raw$inspection_score_min))
scores_raw$inspection_score_median <- as.numeric(as.character(scores_raw$inspection_score_median))
#scores_raw$business_id <- as.character(scores_raw$business_id) #chr did not work out - I see my mistake
str(scores_raw)
```


## Exploratory Analysis
```{r}
#Check tabular look and distribution of values
describe(scores_raw)

# I like the describe function from Hmisc package. It shows detailed info to check for missing, distinct and value distribution. no missing values. There are 1076 distinct values in inspection score mean, I can categorize this as the old scoring system which is a letter grade of A, B, C and D. A represents scores 100 - 91, B represents scores 90 - 81, C 80-71, D below 71. Also note that I am missing a few values in inspection_score_min, median and mean. - Using mean because it is more fair to the restaurant. I however am not going anywhere that is not an A, ever!!! :)
```


```{r}
#Data profiling report
#DataExplorer::create_report(scores_raw)
introduce(scores_raw) #tabular summary version of plot_str
plot_intro(scores_raw) # further detail
plot_missing(scores_raw) # no missing values.

#Visualizing the data with DataExplorer. I see missing values that needs to be taken care of but no discrete columns. looks good at a high level. Now on to data preprocessing.
```

## Data pre-processing

```{r}
library(dplyr)
#I want to look for a better and more simple method to predict the restaurant inspection scores. From the data set, I look at inspection_scores_mean and it ranges from 100 to 50. There are 1076 distinct values.  Too many categories may not be necessary to make a scoring decision, therefore, I will aggregate this variable into groups as the old scoring system which is a letter grade of A, B, C and D. A represents scores 100 - 91, B represents scores 90 - 81, C represents scores 80-71, D represents scores below 71.
#summary(scores_raw$inspection_score_mean) # check the statistics on the classifying column. It is from 1 to 1051 This can be divided into  4 categories.
scores_raw$score <- cut(scores_raw$inspection_score_mean, breaks = c(0,85,90,95,100), labels = c("D","C","B","A")) # 5 breaks for 4 groups
scores_raw$score <- as.factor(scores_raw$score) #make the new column a factor

 # Since I now have the score column, I can remove inspection_score_mean otherwise the output will be associated with inspection_score_mean
inspection_scores <- subset(scores_raw, select = -inspection_score_mean)

#confirm manipulation steps
str(inspection_scores)
table(inspection_scores$score)
#inspection_scores$score
# I think it i am only interested in the scores of D first and then the C prediction. 
# I now have 8 features and 1 factorized classifier. I already treated my data types to be numeric. Although I was able to lower the score variable to 4 categories One thing that is concerning is the distribution of the score variable in the data set. When I divide this into training and test data sets the proportions of the score variable is not going to be cohesive. It will be less efficient for the model to learn from D category then B i would have preferred a data set with a better class distribution. This of course can be manipulated by changing the breaks. It took me a few tries but I was able to divide them, proportionally. 
```

```{r}
#Density plot, and see how Scores are distributed and their correlation to total number of violations. I have been holding on to this chart until the score grouping.
#library(ggplot2)
ggplot(inspection_scores) + aes(total_number_of_violations, color = score) + geom_density()
ggplot(inspection_scores) + aes(total_number_of_violations, fill = score) + geom_density(position = "stack")

#In the first graph, the density plot for total_number_of_violations by score. There is a variation and I am counting on this to help me infer the next score.
#Second graph is simply the stacked version of it.
```
```{r}
inspection_scores<-na.omit(inspection_scores)
#The possible values of score:
unique(inspection_scores$score) 

#Convert possible categorical values to numbers which also converts the class to numeric. And save it to a new variable for modularity.
inspection_scores <- mutate(inspection_scores, score = ifelse(inspection_scores$score == "A", 1,
                         ifelse(inspection_scores$score == "B", 2,
                         ifelse(inspection_scores$score == "C", 3, 4))))
str(inspection_scores)
```

```{r}
inspection_scores[1:8] <- as.data.frame(lapply(inspection_scores[1:8], as.numeric)) 
inspection_scores$score <- as.factor(inspection_scores$score)
str(inspection_scores) # now we have a data set! To come to this point there were many steps, ead, location finders, regex, groupings and aggregations and derived fields. Along the way nlp on violation_descriptions.
```
```{r}
inspection_scores <- subset(inspection_scores, select = -c(inspection_score_median))
str(inspection_scores)
```

```{r}
head(scores_raw)
# Note that the total number of violations are not directly related with the score and that makes it possible to use it as a feature. 
```
```{r}
inspection_scores
# Remember A,B,C,D -> 1,2,3,4
```


```{r}
# Data Explorer is an amazing package that produces highly useful information quickly
#now that the class has changed more eda to see frequency, density, correlation and PCA using DataExplorer package
plot_histogram(inspection_scores)
plot_density(inspection_scores)
plot_correlation(inspection_scores)
# visualize principal component analysis to see variance and relative importance of each feature
plot_prcomp(na.omit(inspection_scores), maxcat = 5L)

plot_intro(inspection_scores)

#density skewness of each variable shows feature relationship.According to correlation results, there is definitely a strong correlation within inspection minimum scores and total number of violations. From PCA it looks like relative importance of each feature is similar. The correlation numbers are not that great and I am not expecting great accuracy results at this point.
```

## Predicting the scores with the kNN algorithm

```{r}
#The reason why i chose to start with k-NN algorithm for this problem:
#First reason is the the nature of the data set i am working with. k nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples. the question asked is to predict the score of restaurants based on past data and my data set is already labeled.
#Second reason is the correlation between features in the data set. k nearest neighbor classifiers are well suited for classification tasks where relationships among the features and the target classes are numerous, complicated, or otherwise extremely difficult to understand, yet the items of similar class type tend to be fairly homogeneous just like the restaurant inspection data set. 
#Third, I have four distinct values for classes. This is a multi-class problem, not binary and kNN is suitable for multi-class problems.
#Finally kNN is considered as one of the simplest ML algorithms. What better way to start things of simple and then build on it. 
```

```{r}
head(inspection_scores, 7)
```

```{r}
#Split data into training and test set using caret. train and test data will have suffix c to specify for caret!
set.seed(99998)  # set seed for random generator 
#use createDataPartition() from caret to split data 70%, 30%
inTrainRows <- createDataPartition(inspection_scores$score,p=0.7,list=FALSE) 
trainData_c <- inspection_scores[inTrainRows,]
testData_c <-  inspection_scores[-inTrainRows,]

trainData_c_labels <- trainData_c$score
testData_c_labels <- testData_c$score

#Although the scores dataset is sorted randomly opting for the random sampling.A thing to note here is that the class variable is not proportionally distributed. adult has ~61%, old has ~34% and young is about 5%. 
#check the split accuracy of _c.
nrow(trainData_c)/(nrow(testData_c)+nrow(trainData_c))
#total number of obs for each of trian and test
dim(trainData_c) 
dim(testData_c)
```

```{r}
 #Now that the train and test ddata sets are ready, one common practice is to begin with a k value equal to the square root of the number of training examples.
sqrt(nrow(trainData_c))

#The result is 59.1 which seems large. But then again i have 3503 observations in my training set. Choosing a large k reduces the impact or variance caused by noisy data but can bias the learner such that it runs the risk of ignoring small, but important patterns. i will start with k=59 and then try a few others for the best accuracy, then pick top 2 for a deeper analysis.
```

## knn - k=59, k=2, k=21, k=53, k=13, k=55
```{r}
#inspection_scores
set.seed(99998) # Setting seed again for repeatable results
inspection_scores_pred <- knn(train = trainData_c[-8], test = testData_c[-8],
                        cl = trainData_c_labels, k = 3)  

#Create a confusion matrix of the predicted versus actual values
score_actual <- testData_c$score
table(inspection_scores_pred, score_actual)

# Compute the accuracy
mean(inspection_scores_pred == score_actual)

#accuracy results:
#k=59 produced 29% 
#k=2 produced 34% 
#k=3 produced 37% 
#k=53 produced 28% 
#k=13 produced 29% 
#k=55 produced 29%

#2 and 3 are the winners however the prediction scores are not the greatest. With 3, I see true positive for D and C collectively are quite correct. I can argue that since the primary goal was to calculate D and C scores then the knn actually works okay. Perhaps I need to classify these only at 2 levels instead of 4. Pass/Fail
```


## Evaluating the knn performance

```{r}
# k=2 and k=3
set.seed(99998) 
inspection_scores_pred_2 <- knn(train = trainData_c[-8], test = testData_c[-8], cl = trainData_c_labels, k = 2)  
inspection_scores_pred_3 <- knn(train = trainData_c[-8], test = testData_c[-8], cl = trainData_c_labels, k = 3)
CrossTable(x = score_actual, y = inspection_scores_pred_2, prop.chisq = FALSE)
CrossTable(x = score_actual, y = inspection_scores_pred_3, prop.chisq = FALSE)

# For k=2. Test data contains 1497 observations. out of which 144 cases have been accurately predicted as A. Also 129 observations were correctly predicted as B (These are "True positives"). For k=3 and k=2 things are similar. Results definitely can be improved. There is no class imbalance problem, I made sure of that earlier. Perhaps k-NN is not as suitable as I initially thought. 

```

```{r}
#Beyond accuracy - other measures of performance
confusionMatrix(score_actual,inspection_scores_pred_2)
confusionMatrix(score_actual,inspection_scores_pred_3)

#34%
```


#Kappa

```{r}
# Let's check the kappa statistic although it should not be too significant since thereint a large class imbalance. Typically a classifier may obtain high accuracy simply by always guessing the most frequent class. The kappa adjusts accuracy by accounting for the possibility of a correct prediction by chance alone. value for k=2 is  0.1231. Anything less then 1 will indicate imperfect agreement.  
#calculate kappa
Kappa(table(score_actual, inspection_scores_pred_2))
Kappa(table(score_actual, inspection_scores_pred_3))
```

```{r}
#Misclassification rate:
mean(inspection_scores_pred_2 != trainData_c$score)
mean(inspection_scores_pred_3 != trainData_c$score)

#the reason for the warning: If the longer object is a multiple of the shorter, R worry that I may have made a mistake, and perhaps didn't mean to perform that comparison, and displays the below warning. My objects are not multiples of each other therefore I am going to ignore the warning. k=2 missclassification rate is higher. A point for choosing k=3.
```

#Sensitivity and specifity
```{r}
#The sensitivity of a model (also called the true positive rate), measures the proportion of positive examples that were correctly classified.
#The specificity of a model (also called the true negative rate), measures the proportion of negative examples that were correctly classified.
#Usually the true negative value is the important value and since the accuracy and sensitivity values of both 2 and 3 are close to each other, both close, both not great.
```

```{r}
#Display confusion matrix as a ggplt graphic while calculating Accuracy, Kappa, Sensitivity and Specificity.
#library(ggplot2)
#library(scales)

cfm_21<-confusionMatrix(score_actual,inspection_scores_pred_2)
cfm_53<-confusionMatrix(score_actual,inspection_scores_pred_3)

ggplotConfusionMatrix <- function(m){
  mytitle <- paste("Accuracy:", percent_format()(m$overall[1]),
                   "Kappa:", percent_format()(m$overall[2]),
                   "Sensitivity:", percent_format()(m$overall[3]),
                   "Specificity:", percent_format()(m$overall[4]))
  p <-
    ggplot(data = as.data.frame(m$table) ,
           aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = log(Freq)), colour = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
    theme(legend.position = "none") +
    ggtitle(mytitle)
  return(p)
}

ggplotConfusionMatrix(cfm_21)
ggplotConfusionMatrix(cfm_53)

```

```{r}
# And finally, obtain the predicted probabilities for k=3.
set.seed(99998) 
inspection_scores_pred_prob<-knn3Train(trainData_c[-8], testData_c[-8], trainData_c_labels, k = 3, prob = TRUE)
inspection_scores_pred_prob_res<-attr(inspection_scores_pred_prob, "prob")

# combine the results into a data frame
inspection_scores_results <- data.frame(actual_type = testData_c_labels,
                          predict_type = inspection_scores_pred,
                          prob_D = inspection_scores_pred_prob_res[ ,4],
                          prob_C = inspection_scores_pred_prob_res[ ,3],
                          prob_B = inspection_scores_pred_prob_res[ ,2],
                          prob_A = inspection_scores_pred_prob_res[ ,1])

# test cases where the model was wrong
head(subset(inspection_scores_results, actual_type != predict_type), n=3)

#Here I used the knn3Train function to return the prediction probabilities for each observation and displayed them side by side with the target classification. All clasess have the same low prediction probability. 
```


## Inspection score prediction with randomForest:

```{r}
#The reasons behind choosing randomForest: inspection_scores data set shows homogeneous data that is linear. Also scores are a sensitive subject for the restaurant owners therefore my method should be transparent for legal reasons and my results should easily be transcribed to be shared with other entities. 
```

```{r}
#inspection_scores set is ready
inspection_scoresV2<-inspection_scores
str(inspection_scoresV2)
```

```{r}
#scale data
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }

inspection_scoresV2[ ,2:7] <- data.frame(lapply(inspection_scoresV2[ ,2:7], scl))
head(inspection_scoresV2, n=3)

#I will check if scaling vs not scaling would make a difference in the result set. Keeping it normalized for now.  (it did not make any difference in accuracy)
```

## Create the traning and test data sets for the models

```{r}
set.seed(99998)  
#70/30
sample_rows3 <- sample(nrow(inspection_scoresV2), nrow(inspection_scoresV2) * 0.70)
# Create the training and test datasets
inspection_scoresV2_train <- inspection_scoresV2[sample_rows3, ]
inspection_scoresV2_test <- inspection_scoresV2[-sample_rows3, ]

# check the proportion of class variable 
round(prop.table(table(inspection_scoresV2_train$score)), 3)
round(prop.table(table(inspection_scoresV2_test$score)), 3)
```

#Decision tree
```{r}
#4 Score class (A, B, C, D): 
# Grow a tree using all of the available data - Building a tree
set.seed(99998)  
m.rpart <- rpart(score~ ., data = inspection_scoresV2, method = "class") # use (.) to select all predictors and method = class for classification
m.rpart # to display basic information

#Not a huge surprise that inspection_score_min is the top predictor. followed by No.Risk and total_number_of_violations.
```


```{r}
#Detailed summary of the tree's fit, including mean squared error for each one of the nodes and overall measure of feature importance.
summary(m.rpart)

#variable importance is predominantly inspection_score_min and then Moderate.Risk along with High.Risk.
```

## Visualizing decision trees 

```{r}
# Here is the graphical representations of the model. 
#plot the scores with fancyRpartPlot that comes with rattle package
fancyRpartPlot(m.rpart)
```


```{r}
# Make predictions on the test dataset
set.seed(99998)  
p.rpart <- predict(m.rpart, inspection_scoresV2_test[,-8], type = "class") 
# Examine the confusion matrix
table(p.rpart, inspection_scoresV2_test$score)
```
```{r}
head(p.rpart)
head(inspection_scoresV2_test$score)
```

```{r}
# Compute the accuracy on the test dataset using mean
mean(p.rpart == inspection_scoresV2_test$score)
#Accuracy is not terrible this time. 85% is not ideal but much better then KNN.
```

```{r}
#Misclassification rate:
suppressWarnings(suppressMessages(mean(p.rpart != inspection_scoresV2_train$score)))
```

```{r}
# Here is more information using ConfusionMatrix
confusionMatrix(p.rpart, inspection_scoresV2_test$score)
# Initially I missed having the predictor as a factor. I am still such a rookie!
```

```{r}
# Cross tabulation of predicted versus actual classes
#library(gmodels)
CrossTable(inspection_scoresV2_test$score, p.rpart,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, #prop.c & prop.r= FALSE removes column & row percentages
           dnn = c('actual score', 'predicted score'))
```

## Prune the trees.

```{r}
# I am hoping that by pruning the classification tree I can avoid over-fitting and even maybe produce a more robust classification model.
#First find cp
printcp(m.rpart) #- minimum xerror
min(m.rpart$cptable[,"xerror"]) #find minimum, cross validation error of the classification tree
opt <- which.min(m.rpart$cptable[,"xerror"]) #locate the record with the minimum cross validation errors.
cp <- m.rpart$cptable[opt,"CP"]  # get the cost complexity parameter of the record with the minimum cross validation errors.
cp  
```

```{r}
# Prune
set.seed(99998)  
mpru.rpart <- prune(m.rpart, cp) #cp is prune functions complexity parameter, to create a simpler pruned tree. This value was calculated above
mpru.rpart 

## no terminal nodes were pruned! I tried other cp values and was able to cut the tree, however this did not result in a massive accuracy change. The pruned model plot got smaller.
```


```{r}
# Plot the pruned model
fancyRpartPlot(mpru.rpart)
```


```{r}
#Predict - pruned tree
set.seed(99998)  
ppru.rpart <- predict(mpru.rpart, inspection_scoresV2_test[,-8], type = "class") 
# Examine the confusion matrix
table(ppru.rpart, inspection_scoresV2_test$score)
#results are identical to the non pruned version. Expected since there was no pruning of the trees. The pruning is essentially a method of limiting tree depth to reduce over fitting.
```


```{r}
# Compute the accuracy on the test dataset using mean
mean(ppru.rpart == inspection_scoresV2_test$score)
```

```{r}
#Misclassification rate:
suppressWarnings(suppressMessages(mean(ppru.rpart != inspection_scoresV2_test$score)))
```

```{r}
# Here is more information using ConfusionMatrix
confusionMatrix(inspection_scoresV2_test$score,ppru.rpart)

#Pruned results analysis:
#Pruning should reduce the size of the decision tree which reduces training accuracy therefore improve the accuracy on test (unseen) data. So, pruning should improve testing accuracy. However for overall accuracy in my runs it does not. This makes me think I need more training data. Maybe I can increase accuracy by changing the class variable to a binary choice. Although I dont have a class imbalance problem, I think lack of enough features is making multi-class inference a harder task.
```

#Pruning the tree heuristically

```{r}
#I wanted to experiment with other cp values since pruning is usally done by heuristics.  My question is: If I chose a different CP value, can my accuracy be improved?
#printcp(m.rpart) # to get CP values.
set.seed(99998)  
mpru.rpart_try <- prune(m.rpart, 0.05) 
fancyRpartPlot(mpru.rpart_try)
ppru.rpart_try <- predict(mpru.rpart_try, inspection_scoresV2_test[,-8], type = "class") 
mean(ppru.rpart_try == inspection_scoresV2_test$score)

set.seed(99998)  
mpru.rpart_try2 <- prune(m.rpart, 0.09) #just using shell.weight to make the prediction
fancyRpartPlot(mpru.rpart_try2)
ppru.rpart_try2 <- predict(mpru.rpart_try2, inspection_scoresV2_test[,-8], type = "class") 
mean(ppru.rpart_try2 == inspection_scoresV2_test$score)
#Accuracy results are identical. cp value represents the cost of adding node to the tree. the default value 0.01 seems like the the best choice. 
```

## Train, predict, and evaluate the performance using randomForests

### One way to evaluate the performance of a model is to train it on a number of different smaller datasets and evaluate them over the other smaller testing set. This is called the F-fold cross-validation feature. R has a function to randomly split number of datasets of almost the same size. For example, if k=9, the model is evaluated over the nine folder and tested on the remaining test set. This process is repeated until all the subsets have been evaluated. This technique is widely used for model selection, especially when the model has parameters to tune. Now that we have a way to evaluate our model, we need to figure out how to choose the parameters that generalized best the data. Random forest chooses a random subset of features and builds many Decision Trees. The model averages out all the predictions of the Decisions trees.
```{r}
set.seed(99998)  
fit <- randomForest(score ~., data=inspection_scoresV2_train)
print(fit) # view results
#Here, I also tried using entire data as the training data. I am confident that if larger amount of data is used and classifier is set to binary mode, the accuracy would be much higher. Also another thing to note is the class error. Earlier I mentioned I am interested in mostly the D and C scores. Although the rate is acceptable for D the error rate for C is a bit significant.
```


```{r}
varImpPlot(fit)
varImp(fit) # importance of each predictor
#number of trees build is 500. score min is the biggest predictor followed by the rest equally. As future work I can remove the inspection_score_min and see if it would perform a bit more uniform for classes C and D. 
```

```{r}
#predict decision tree
set.seed(99998)  
fit_predict <- predict(fit,inspection_scoresV2_test[,-8],type="class")
table(fit_predict, inspection_scoresV2_test$score)
#Results are already looking much better then before. Slight overlap between C and D (3 and 4) 
```
```{r}
#How to get my list of predicted numbers.
#fit_predict
#fit_predict[[1]]
probs <- predict(fit,inspection_scoresV2[,-8],type="prob")
head(probs)
```

```{r}
# Add predictions back to the table
#predict(fit,inspection_scoresV2[,-8],type="class")
library(data.table)
setDT(inspection_scoresV2)
inspection_scoresV2[ , predictions := predict(fit,inspection_scoresV2[,-8],type="class")]
inspection_scoresV2
inspection_scoresV3 = subset(inspection_scoresV2, select = c('business_id','score', 'predictions'))
inspection_scoresV3
```
```{r}
# Update back to letters
inspection_scoresV4<-inspection_scoresV3
inspection_scoresV4$score <- ifelse(inspection_scoresV4$score == 1, "A", ifelse(inspection_scoresV4$score == 2, "B", ifelse(inspection_scoresV4$score == 3, "C", "D")) ) #using if else to replace

inspection_scoresV4$predictions <- ifelse(inspection_scoresV4$predictions == 1, "A", ifelse(inspection_scoresV4$predictions == 2, "B", ifelse(inspection_scoresV4$predictions == 3, "C", "D")) ) 
inspection_scoresV4
```

```{r}
# Write back to csv and export out
write.csv(inspection_scoresV4,"/cloud/project/Week7/business_id_predictions.csv", row.names = FALSE)
```

```{r}
#Calculate Accuracy:
(316+348+321+360) / nrow(inspection_scoresV2_test)

# A 4% improvement with randomforest. The accuracy is at 90% which is an acceptable value.
```

```{r}
#here is all the results combined in a single cell to compare side by side
table(inspection_scoresV2_test$score, p.rpart,  dnn = "decisiontree") #decision tree
mean(p.rpart == inspection_scoresV2_test$score) #Accuracy
table(inspection_scoresV2_test$score, ppru.rpart, dnn = "pruned") #pruned
mean(ppru.rpart == inspection_scoresV2_test$score) #Accuracy
table(inspection_scoresV2_test$score, fit_predict, dnn = "randomforest") #random forest
mean(fit_predict == inspection_scoresV2_test$score) #Accuracy

# No improvement with pruning however some improvement with random forest method. I suspect the class distribution and the way I prepped the data also helped achieve the better accuracy.
```

```{r}
#CrossTable function to display the percentage distribution
CrossTable(inspection_scoresV2_test$score, p.rpart,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, #prop.c & prop.r= FALSE removes column & row percentages
           dnn = c('actual score', 'predicted score'))

CrossTable(inspection_scoresV2_test$score, ppru.rpart,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, #prop.c & prop.r= FALSE removes column & row percentages
           dnn = c('actual score', 'predicted score'))

CrossTable(inspection_scoresV2_test$score, fit_predict,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, #prop.c & prop.r= FALSE removes column & row percentages
           dnn = c('actual score', 'predicted score'))

#The non pruned and pruned return identical results. And more interestingly both produce better results then the random forest. 
```


```{r}
#ConfusionMatrix to assess performance measures other then accuracy.
confusionMatrix(p.rpart, inspection_scoresV2_test$score)
confusionMatrix(ppru.rpart, inspection_scoresV2_test$score)
confusionMatrix(fit_predict, inspection_scoresV2_test$score)
#The sensitivity of a model (also called the true positive rate), measures the proportion of positive examples that were correctly classified. Here for class A the sensitivity is larger then classes B, C and D
#The specificity of a model (also called the true negative rate), measures the proportion of negative examples that were correctly classified. The values are very good for Class D and Class A. This is one of the reasons, I think the classifier can perform better with a boolean predictor however that is not my goal here, my goal is to succsfully sperate D's first and then C's.
#The results for decision tree and pruned tree are identical. Random Forest is a bit higher. The decision tree also scored much better then the k-nn algorithm. Also normalizing vs not did not have any effect on the results. Why the pruning is not improving the results could be many. As a default, the model is using 0.01 for the CP value. It is minimum cross-validated error tree. To test, I changed the cp value manually however was  unable to improve accuracy.

#I think if knn and decision tree were used in another dataset the accuracy will be higher and also different from each other. I have used both methods with a few other data sets and especially with the decision tree the accuracy was never less then 81%. This makes me think that the class imbalance in the target variable might not be allowing the methods train properly. Maybe when i split training and test data sets, I dont have any relatable values in test to check the accuracy properly. Thinking this I also did runs increasing my training vs test data set distribution. I tried 60/40, 75/25, and 90/10 splits. With 90/10 split, accuracy increased slightly and both knn and decision trees still produced similar performance.
```


## Estimating model performance (k-fold cross validation)

# randomForest
```{r}
#Starting with randomForest
#check data
str(inspection_scoresV2) #normalized dataset was used for randomForest example. Normalization vs not did not make a difference.
```


```{r}
## Automating 10-fold CV for a randomForest using lapply() 
#Using the 10 fold CV since it is the most common convention. There is little added benefit to using a greater number. For each of the 10 folds a ML model is built on the remaining 90% of data. The fold's 10% sample is then used for model evaluation. After the process of training and evaluating the model has occurred 10 times the average performance across all folds is reported.

set.seed(99998)
folds <- createFolds(inspection_scoresV2$score, k = 10) #command to create 10 folds. Datasets for CV are created. createFolds is part of caret function. 

cv_results <- lapply(folds, function(x) {
  inspection_scoresV2_train <- inspection_scoresV2[-x, ]
  inspection_scoresV2_test <- inspection_scoresV2[x, ]
  inspection_scoresV2_model <- randomForest(score ~ ., data = inspection_scoresV2_train)
  inspection_scoresV2_pred <- predict(inspection_scoresV2_model, inspection_scoresV2_test)
  inspection_scoresV2_actual <- inspection_scoresV2_test$score
  kappa <- kappa2(data.frame(inspection_scoresV2_actual, inspection_scoresV2_pred))$value
  return(kappa)
})

#This took a couple of minutes to run. expected. The whole idea of cross validation is to optimize the model complexity while minimizing under/overfitting. The goal is for the model to generalize well without overfitting. To ensure this, the model is (cross) validated / qualified.
```

```{r}
str(folds) #results of the createFolds() function.List if vectors storing the row numbers for each of the k=10 requested folds. 
str(cv_results)
mean(unlist(cv_results))
#using the unlist() function, which eliminates the list structure and reduces cv_results to a numeric vector. From there, calculate the mean kappa as expected. Result is lower then before.  k-fold did not work as intended for randomForest.
```

#knn
```{r}
str(inspection_scores) #non normalized dataset
```

```{r}
## Automating 10-fold CV for knn
set.seed(99998)  
trControl <- trainControl(method  = "cv",
                          number  = 10)
#Then you can evaluate the accuracy of the KNN classifier with different values of k by cross validation using
fit <- train(score ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = inspection_scores)
fit

#Slight improvement in the results. Accuracy is a little better using 10-fold cv for knn. 
```

```{r}
#Summary:
#I decided on the knn and decision trees after the explanatory analysis on the data. I thought the data was suitable to work with these methods. During my research initially, I read that "Cross-validation is not necessary when using random forest, because multiple bagging in process of training random forest prevents over-fitting. But then I learned that cross validation isn't used for building or pruning the decision trees. It is used to estimate how good the tree built on all of the data will perform by simulating arrival of new data (by building the tree without some elements). It doesn't really make sense to pick one of the trees generated by it because the model is constrained by the data you have. So basically the purpose of cross validation is done to qualify the model which can in return tell me the level of precision I can expect from its application.

#During cross validation, the little amount of training data is divided into "folds" which can lead to trees which are either overfit or underfit for particular data instances. Typically a class imbalance is the root cause of cross validation not performing as expected. If a class is a small proportion of the dataset, it can be omitted from the training data set. Fort his reason, the model cannot then learn from this class. To confirm the effect of this, I also ran the model with all the data instead of just the training subset and did get a better tree. 
```




